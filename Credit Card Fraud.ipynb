{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"creditcard.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x250799fd940>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let us see target Class distribution\n",
    "sns.countplot(x=data.Class, data= data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#totally skewed or imbalanced. Lets check the number of rows to decide on resampling.\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns = 'Time',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying undersampling as amount of data is too much\n",
    "\n",
    "#taking count of rows that are fraudulant\n",
    "no_frauds = len(data[data.Class == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_frauds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking number of Non fraudulant indicies\n",
    "non_fraud_indices = data[data.Class == 0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking 492 rows by indices randomly\n",
    "random_indices = np.random.choice(non_fraud_indices,no_frauds, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take indies of fraudulant data\n",
    "fraud_indices = data[data.Class == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a combination of 492 Fradulant and 492 non Fraudulant data\n",
    "under_sample_indices = np.concatenate([fraud_indices,random_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   541,    623,   4920,   6108,   6329,   6331,   6334,   6336,\n",
       "         6338,   6427,   6446,   6472,   6529,   6609,   6641,   6717,\n",
       "         6719,   6734,   6774,   6820,   6870,   6882,   6899,   6903,\n",
       "         6971,   8296,   8312,   8335,   8615,   8617,   8842,   8845,\n",
       "         8972,   9035,   9179,   9252,   9487,   9509,  10204,  10484,\n",
       "        10497,  10498,  10568,  10630,  10690,  10801,  10891,  10897,\n",
       "        11343,  11710,  11841,  11880,  12070,  12108,  12261,  12369,\n",
       "        14104,  14170,  14197,  14211,  14338,  15166,  15204,  15225,\n",
       "        15451,  15476,  15506,  15539,  15566,  15736,  15751,  15781,\n",
       "        15810,  16415,  16780,  16863,  17317,  17366,  17407,  17453,\n",
       "        17480,  18466,  18472,  18773,  18809,  20198,  23308,  23422,\n",
       "        26802,  27362,  27627,  27738,  27749,  29687,  30100,  30314,\n",
       "        30384,  30398,  30442,  30473,  30496,  31002,  33276,  39183,\n",
       "        40085,  40525,  41395,  41569,  41943,  42007,  42009,  42473,\n",
       "        42528,  42549,  42590,  42609,  42635,  42674,  42696,  42700,\n",
       "        42741,  42756,  42769,  42784,  42856,  42887,  42936,  42945,\n",
       "        42958,  43061,  43160,  43204,  43428,  43624,  43681,  43773,\n",
       "        44001,  44091,  44223,  44270,  44556,  45203,  45732,  46909,\n",
       "        46918,  46998,  47802,  48094,  50211,  50537,  52466,  52521,\n",
       "        52584,  53591,  53794,  55401,  56703,  57248,  57470,  57615,\n",
       "        58422,  58761,  59539,  61787,  63421,  63634,  64329,  64411,\n",
       "        64460,  68067,  68320,  68522,  68633,  69498,  69980,  70141,\n",
       "        70589,  72757,  73784,  73857,  74496,  74507,  74794,  75511,\n",
       "        76555,  76609,  76929,  77099,  77348,  77387,  77682,  79525,\n",
       "        79536,  79835,  79874,  79883,  80760,  81186,  81609,  82400,\n",
       "        83053,  83297,  83417,  84543,  86155,  87354,  88258,  88307,\n",
       "        88876,  88897,  89190,  91671,  92777,  93424,  93486,  93788,\n",
       "        94218,  95534,  95597,  96341,  96789,  96994,  99506, 100623,\n",
       "       101509, 102441, 102442, 102443, 102444, 102445, 102446, 102782,\n",
       "       105178, 106679, 106998, 107067, 107637, 108258, 108708, 111690,\n",
       "       112840, 114271, 116139, 116404, 118308, 119714, 119781, 120505,\n",
       "       120837, 122479, 123141, 123201, 123238, 123270, 123301, 124036,\n",
       "       124087, 124115, 124176, 125342, 128479, 131272, 135718, 137705,\n",
       "       140786, 141257, 141258, 141259, 141260, 142405, 142557, 143188,\n",
       "       143333, 143334, 143335, 143336, 143728, 143731, 144104, 144108,\n",
       "       144754, 145800, 146790, 147548, 147605, 149145, 149357, 149522,\n",
       "       149577, 149587, 149600, 149869, 149874, 150601, 150644, 150647,\n",
       "       150654, 150660, 150661, 150662, 150663, 150665, 150666, 150667,\n",
       "       150668, 150669, 150677, 150678, 150679, 150680, 150684, 150687,\n",
       "       150692, 150697, 150715, 150925, 151006, 151007, 151008, 151009,\n",
       "       151011, 151103, 151196, 151462, 151519, 151730, 151807, 152019,\n",
       "       152223, 152295, 153823, 153835, 153885, 154234, 154286, 154371,\n",
       "       154454, 154587, 154633, 154668, 154670, 154676, 154684, 154693,\n",
       "       154694, 154697, 154718, 154719, 154720, 154960, 156988, 156990,\n",
       "       157585, 157868, 157871, 157918, 163149, 163586, 167184, 167305,\n",
       "       172787, 176049, 177195, 178208, 181966, 182992, 183106, 184379,\n",
       "       189587, 189701, 189878, 190368, 191074, 191267, 191359, 191544,\n",
       "       191690, 192382, 192529, 192584, 192687, 195383, 197586, 198868,\n",
       "       199896, 201098, 201601, 203324, 203328, 203700, 204064, 204079,\n",
       "       204503, 208651, 212516, 212644, 213092, 213116, 214662, 214775,\n",
       "       215132, 215953, 215984, 218442, 219025, 219892, 220725, 221018,\n",
       "       221041, 222133, 222419, 223366, 223572, 223578, 223618, 226814,\n",
       "       226877, 229712, 229730, 230076, 230476, 231978, 233258, 234574,\n",
       "       234632, 234633, 234705, 235616, 235634, 235644, 237107, 237426,\n",
       "       238222, 238366, 238466, 239499, 239501, 240222, 241254, 241445,\n",
       "       243393, 243547, 243699, 243749, 243848, 244004, 244333, 245347,\n",
       "       245556, 247673, 247995, 248296, 248971, 249167, 249239, 249607,\n",
       "       249828, 249963, 250761, 251477, 251866, 251881, 251891, 251904,\n",
       "       252124, 252774, 254344, 254395, 255403, 255556, 258403, 261056,\n",
       "       261473, 261925, 262560, 262826, 263080, 263274, 263324, 263877,\n",
       "       268375, 272521, 274382, 274475, 275992, 276071, 276864, 279863,\n",
       "       280143, 280149, 281144, 281674, 268151,  11153, 222878,  10353,\n",
       "       246324, 276843,  29347, 224299,  76491, 156244, 254700, 261134,\n",
       "        99524, 170252,  87126,  95145, 238059,   8971,  62685,  48985,\n",
       "        88291, 111072, 164377, 250051, 180070,  59691, 167277, 225755,\n",
       "        79222, 237468,  46599, 192994,  57236,  90465,  81227, 140256,\n",
       "       183182,   5614,  15976, 128128,  44470,  21425, 191588, 239303,\n",
       "        55998, 258387, 127904,  75487, 150787, 279988, 150356,  53469,\n",
       "        66075,  22044, 158897,  86896, 209373, 123078, 262535, 185590,\n",
       "       228767, 168058, 165759,  69719, 182717,  50333, 276064, 247480,\n",
       "        92710, 109937, 166929,  21924, 189035, 123816, 133552, 153419,\n",
       "        56367, 189733, 148450,  48792, 200962,  42142, 256536, 103048,\n",
       "       114257,  75799, 194031,  53143, 173361, 128566,  64890, 185637,\n",
       "       202133,  40148, 264033,  42052,  15336,  28348,  56626, 146238,\n",
       "       236428,  13093, 146280, 267198, 196160, 243957, 188347, 151129,\n",
       "       279457, 241986, 113113, 130803,  26706, 199313,  91013, 247701,\n",
       "       185190,  40371,  12194, 147084,  27311, 224640,  13857,  80049,\n",
       "       130558,  75184, 205571, 240647, 216018,  24982,  25517, 136906,\n",
       "       119157,  83826, 282159, 200988,  73163, 175763, 181771, 243387,\n",
       "        83540, 153681, 154943,  93999,  20376,  61751, 112101, 225227,\n",
       "       239172,  25817,  26848, 225971, 192406, 170685,  36311,  93068,\n",
       "       233916, 119891,  35317,  14329, 167390,  63847,   3946, 204524,\n",
       "        84146, 276661, 160637, 241240, 125481, 265750, 242161, 257342,\n",
       "       248217,  24276, 194748, 170497, 213128, 262457, 240218, 274713,\n",
       "       170626, 188556,  45828, 141780,  12502, 237650, 243923,  43842,\n",
       "        40561,  55020, 115919,  98369, 222896,  53375, 212350,  27581,\n",
       "       186818,  12596, 177080, 241903, 146281, 279082, 283842, 225646,\n",
       "       181232,  68999,  62791, 144314,  70076,  72355, 260013, 242116,\n",
       "        56237, 194309, 149406, 168084,  68734, 269840, 249092,  60859,\n",
       "       228274, 235298,  42004, 243175,  22391,  76597,  53035, 109801,\n",
       "        13631, 147949, 282817, 276716,  71903,  21493,  79932, 262626,\n",
       "        29257, 282731,  80033, 263676, 189729,  40555,   6081, 113481,\n",
       "         8259, 239221, 260559,  65169,  62057, 198683, 171351, 142268,\n",
       "        63428, 133770, 117189,  30597, 144066, 132393, 109011,  43657,\n",
       "        98964, 176225, 232917, 135634,  68689,  80591, 138721, 201305,\n",
       "       128241, 246264, 268837,  33485,  12823, 222243, 111995, 165315,\n",
       "       124552, 124691, 252221, 147641,  68650, 116575, 188192, 235118,\n",
       "        65209,  15852,  33134, 121102, 140812, 109631, 249850, 127404,\n",
       "        35227,  12389, 277060,  88138,   8982, 162954, 189300,  75275,\n",
       "        96012, 192811, 243594,  22653, 241363, 187747,  78658, 269697,\n",
       "       129943, 218458, 191115,  31904,  63052, 172099, 115418,  94631,\n",
       "       108247, 194321,  58360, 243399, 233769, 176042, 229150,   5691,\n",
       "       282984,  43263, 152977, 242174, 218037, 157581, 185963,   7468,\n",
       "       136431, 283189, 136624, 249379, 110807,  51319, 196410,  89636,\n",
       "       174557, 276714,  29859,  78478,  20825, 117988, 215168, 251246,\n",
       "       125423,  30438, 163506,  37056,  53148, 152107, 196458, 230888,\n",
       "        45823, 267387, 119348, 241548, 263025, 244500, 132511, 241764,\n",
       "       146655, 215077, 182295, 127558, 136914, 273001, 163055,  48027,\n",
       "        52069,  35567,  80637,  12705,   7828, 268773, 219063, 269086,\n",
       "        49668, 218114,  73797,  17671, 146890, 218213,  95863,  38051,\n",
       "        43340, 186348,  33366, 248190, 190060, 102363,  64898, 192054,\n",
       "         8927, 184475,  88658,  62664, 179185,  18888,  48977, 167939,\n",
       "        23021, 267196,  92563, 181516,  27399, 178719, 179726, 191111,\n",
       "       235804,  75743,  58884, 149828, 127517, 162366, 135639, 258653,\n",
       "        35948,  17902,  91634,  29534,  61175,  11031, 166749, 147959,\n",
       "       210953,  17465, 275673, 115752, 233224, 235803,  61743, 191965,\n",
       "        94054,  13753,   4065,  36568,   1188, 186362, 203940,  11923,\n",
       "        65424,  73966, 252913,  68595, 157439, 245552, 211546, 257674,\n",
       "       169408,  27422, 261453,  95447, 186084,  77806,  26915, 214398,\n",
       "       116611,  85054, 188034,  97713,  56433, 219688, 176967,  56148,\n",
       "       212812, 215768,  37952, 196158, 184092, 152784, 255989,  10065,\n",
       "        96952, 272407, 273624, 108583, 112682, 180803, 276989, 179678,\n",
       "       189697, 123136,   1380,  58849, 228556, 265347,  35216,  35533],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_sample_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "under_sample = data.loc[under_sample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the data 80, 20 ratio for training and test \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_under = under_sample.loc[:,under_sample.columns != 'Class']\n",
    "y_under = under_sample.loc[:,under_sample.columns == 'Class']\n",
    "X_under_train, X_under_test, y_under_train, y_under_test = train_test_split(X_under,y_under,test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#trying Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_under = LogisticRegression()\n",
    "lr_under.fit(X_under_train,y_under_train)\n",
    "y_under_pred = lr_under.predict(X_under_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.9504447268106735\n",
      "Test Score:  0.9390862944162437\n",
      "0.945054945054945\n",
      "0.9390862944162437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.90243902, 0.92307692, 0.97435897, 0.94871795, 0.94871795])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calulate the R , accuracy and recall scores and check via cross validation\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "print(\"Training Score: \", lr_under.score(X_under_train,y_under_train))\n",
    "print(\"Test Score: \", lr_under.score(X_under_test,y_under_test))\n",
    "print(recall_score(y_under_test,y_under_pred))\n",
    "print(accuracy_score(y_under_test,y_under_pred))\n",
    "cross_val_score(lr_under, X_under_test, y_under_test, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## That was good, now lets try to apply the same model to original data and then predict\n",
    "\n",
    "X = data.loc[:,data.columns != 'Class']\n",
    "y = data.loc[:,data.columns == 'Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_under.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:  0.9665737672540543\n",
      "Test Score:  0.9660475404655735\n",
      "0.9603960396039604\n",
      "0.9660475404655735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.99907834, 0.99923193, 0.99920999, 0.99927582, 0.99912219])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training Score: \", lr_under.score(X_train,y_train))\n",
    "print(\"Test Score: \", lr_under.score(X_test,y_test))\n",
    "print(recall_score(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "cross_val_score(lr_under, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9306930693069307\n",
      "0.9762473227765879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\bharath\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.98551869, 0.97744031, 0.97164677, 0.98139045, 0.97998596])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scores were good but looks like cross validation appears to be too good to be true.\n",
    "\n",
    "#Trying weighted logistic regression om original data and checking all scores again\n",
    "\n",
    "lr_balanced = LogisticRegression(class_weight = 'balanced')\n",
    "lr_balanced.fit(X_train,y_train)\n",
    "y_balanced_pred = lr_balanced.predict(X_test)\n",
    "print(recall_score(y_test,y_balanced_pred))\n",
    "print(accuracy_score(y_test,y_balanced_pred))\n",
    "cross_val_score(lr_balanced, X_test, y_test, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "Looks like Logistic Regression with under sampling or the weighted balance method can be the answer when we have such imbalanced dataset. As we can see, though I have used undersampling, I had made sure to check that on original data as well to make sure we are not missing any important information and also used cross validation to recheck our results. On an average we see that the score is 95% despite of the method we use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
