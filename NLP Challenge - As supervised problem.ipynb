{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP Challenge - As supervised problem.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"aVhA88fhPfH2","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","import numpy as np\n","import pandas as pd\n","import scipy\n","import sklearn\n","import spacy\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","from nltk.corpus import gutenberg, stopwords\n","from collections import Counter"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WD_tmdelPnJm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"2d158fa8-9441-4082-fe0f-b7d9c1e54748","executionInfo":{"status":"ok","timestamp":1551124835725,"user_tz":300,"elapsed":769,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["import nltk\n","nltk.download('gutenberg')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/gutenberg.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"metadata":{"id":"8Cg0m4rgPqoK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"ca0a4334-ae82-44dd-b4d3-8c28648b054b","executionInfo":{"status":"ok","timestamp":1551124847167,"user_tz":300,"elapsed":331,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"metadata":{"id":"p9yWNUXDPvcq","colab_type":"text"},"cell_type":"markdown","source":["Supervised NLP requires a pre-labelled dataset for training and testing, and is generally interested in categorizing text in various ways. In this case, we are going to try to predict whether a sentence comes from _Alice in Wonderland_ by Lewis Carroll or _Persuasion_ by Jane Austen. We can use any of the supervised models we've covered previously, as long as they allow categorical outcomes. In this case, we'll try Random Forests, SVM, and KNN.\n","\n","Our feature-generation approach will be something called _BoW_, or _Bag of Words_. BoW is quite simple: For each sentence, we count how many times each word appears. We will then use those counts as features.  "]},{"metadata":{"id":"h75eLy4mPtiB","colab_type":"code","colab":{}},"cell_type":"code","source":["# Utility function for standard text cleaning.\n","def text_cleaner(text):\n","    # Visual inspection identifies a form of punctuation spaCy does not\n","    # recognize: the double dash '--'.  Better get rid of it now!\n","    text = re.sub(r'--',' ',text)\n","    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n","    text = ' '.join(text.split())\n","    return text\n","    \n","# Load and clean the data.\n","persuasion = gutenberg.raw('austen-persuasion.txt')\n","alice = gutenberg.raw('carroll-alice.txt')\n","\n","# The Chapter indicator is idiosyncratic\n","persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n","alice = re.sub(r'CHAPTER .*', '', alice)\n","\n","alice = text_cleaner(alice)\n","persuasion = text_cleaner(persuasion)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"I47qK4JhPxZi","colab_type":"code","colab":{}},"cell_type":"code","source":["# Parse the cleaned novels. This can take a bit.\n","nlp = spacy.load('en')\n","alice_doc = nlp(alice)\n","persuasion_doc = nlp(persuasion)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FUeO9osGQSOJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"c365d905-f94c-4a88-84e4-c0d840b6a5cc","executionInfo":{"status":"ok","timestamp":1551125077283,"user_tz":300,"elapsed":420,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["# Group into sentences.\n","alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n","persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n","\n","# Combine the sentences from the two novels into one data frame.\n","sentences = pd.DataFrame(alice_sents + persuasion_sents)\n","sentences.head()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n","      <td>Carroll</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>(So, she, was, considering, in, her, own, mind...</td>\n","      <td>Carroll</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n","      <td>Carroll</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>(Oh, dear, !)</td>\n","      <td>Carroll</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>(I, shall, be, late, !, ')</td>\n","      <td>Carroll</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                   0        1\n","0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n","1  (So, she, was, considering, in, her, own, mind...  Carroll\n","2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n","3                                      (Oh, dear, !)  Carroll\n","4                         (I, shall, be, late, !, ')  Carroll"]},"metadata":{"tags":[]},"execution_count":6}]},{"metadata":{"id":"soZ2sSzlQpYB","colab_type":"text"},"cell_type":"markdown","source":["Time to bag some words!  Since spaCy has already tokenized and labelled our data, we can move directly to recording how often various words occur.  We will exclude stopwords and punctuation.  In addition, in an attempt to keep our feature space from exploding, we will work with lemmas (root words) rather than the raw text terms, and we'll only use the 2000 most common words for each text."]},{"metadata":{"id":"3z09BypPQlr5","colab_type":"code","colab":{}},"cell_type":"code","source":["# Utility function to create a list of the 2000 most common words.\n","def bag_of_words(text):\n","    \n","    # Filter out punctuation and stop words.\n","    allwords = [token.lemma_\n","                for token in text\n","                if not token.is_punct\n","                and not token.is_stop]\n","    \n","    # Return the most common words.\n","    return [item[0] for item in Counter(allwords).most_common(2000)]\n","   \n","# Set up the bags.\n","alicewords = bag_of_words(alice_doc)\n","persuasionwords = bag_of_words(persuasion_doc)\n","\n","# Combine bags to create a set of unique words.\n","common_words = set(alicewords + persuasionwords)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iHtOKi3aQtZS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5c2f29ec-975b-4fd3-8e55-283af654c0e0","executionInfo":{"status":"ok","timestamp":1551125127700,"user_tz":300,"elapsed":344,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["list(common_words)[0]"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'worthy'"]},"metadata":{"tags":[]},"execution_count":8}]},{"metadata":{"id":"cZdhhlQiQyAM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":250},"outputId":"0bbace87-db70-41a0-ea84-68b3edb5e6af","executionInfo":{"status":"ok","timestamp":1551125194837,"user_tz":300,"elapsed":918,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer(vocabulary=common_words)\n","df = pd.DataFrame()\n","df['text_sentence'] = sentences[0]\n","df['text_source'] = sentences[1]\n","#String version of the spacy objects\n","df['str_sentence'] = [_.text for _ in df.text_sentence]\n","\n","X = vectorizer.fit_transform(df['str_sentence'])\n","\n","df2 =  pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n","\n","word_counts = pd.concat([df,df2], axis=1)\n","print(word_counts.shape)\n","word_counts.head()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["(5318, 3065)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text_sentence</th>\n","      <th>text_source</th>\n","      <th>str_sentence</th>\n","      <th>'s</th>\n","      <th>-PRON-</th>\n","      <th>1</th>\n","      <th>`</th>\n","      <th>a</th>\n","      <th>abide</th>\n","      <th>ability</th>\n","      <th>...</th>\n","      <th>you</th>\n","      <th>you'd</th>\n","      <th>young</th>\n","      <th>your</th>\n","      <th>yours</th>\n","      <th>youth</th>\n","      <th>zeal</th>\n","      <th>zealand</th>\n","      <th>zealous</th>\n","      <th>zigzag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n","      <td>Carroll</td>\n","      <td>Alice was beginning to get very tired of sitti...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>(So, she, was, considering, in, her, own, mind...</td>\n","      <td>Carroll</td>\n","      <td>So she was considering in her own mind (as wel...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n","      <td>Carroll</td>\n","      <td>There was nothing so VERY remarkable in that; ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>(Oh, dear, !)</td>\n","      <td>Carroll</td>\n","      <td>Oh dear!</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>(I, shall, be, late, !, ')</td>\n","      <td>Carroll</td>\n","      <td>I shall be late!'</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 3065 columns</p>\n","</div>"],"text/plain":["                                       text_sentence text_source  \\\n","0  (Alice, was, beginning, to, get, very, tired, ...     Carroll   \n","1  (So, she, was, considering, in, her, own, mind...     Carroll   \n","2  (There, was, nothing, so, VERY, remarkable, in...     Carroll   \n","3                                      (Oh, dear, !)     Carroll   \n","4                         (I, shall, be, late, !, ')     Carroll   \n","\n","                                        str_sentence  's  -PRON-  1  `  a  \\\n","0  Alice was beginning to get very tired of sitti...   0       0  0  0  0   \n","1  So she was considering in her own mind (as wel...   0       0  0  0  0   \n","2  There was nothing so VERY remarkable in that; ...   0       0  0  0  0   \n","3                                           Oh dear!   0       0  0  0  0   \n","4                                  I shall be late!'   0       0  0  0  0   \n","\n","   abide  ability   ...    you  you'd  young  your  yours  youth  zeal  \\\n","0      0        0   ...      0      0      0     0      0      0     0   \n","1      0        0   ...      0      0      0     0      0      0     0   \n","2      0        0   ...      0      0      0     0      0      0     0   \n","3      0        0   ...      0      0      0     0      0      0     0   \n","4      0        0   ...      0      0      0     0      0      0     0   \n","\n","   zealand  zealous  zigzag  \n","0        0        0       0  \n","1        0        0       0  \n","2        0        0       0  \n","3        0        0       0  \n","4        0        0       0  \n","\n","[5 rows x 3065 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"metadata":{"id":"t0SqtCRWRG-4","colab_type":"text"},"cell_type":"markdown","source":[" ## Trying out BoW\n","\n","Now let's give the bag of words features a whirl by trying a random forest."]},{"metadata":{"id":"uvacFZ6vRCQz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"4204461a-eda1-4c2b-ebf6-64ecac50bf8a","executionInfo":{"status":"ok","timestamp":1551125235079,"user_tz":300,"elapsed":1538,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["from sklearn import ensemble\n","from sklearn.model_selection import train_test_split\n","\n","rfc = ensemble.RandomForestClassifier()\n","Y = word_counts['text_source']\n","X = np.array(word_counts.drop(['text_sentence','text_source','str_sentence'], 1))\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, \n","                                                    Y,\n","                                                    test_size=0.4,\n","                                                    random_state=0)\n","train = rfc.fit(X_train, y_train)\n","\n","print('Training set score:', rfc.score(X_train, y_train))\n","print('\\nTest set score:', rfc.score(X_test, y_test))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Training set score: 0.9846394984326019\n","\n","Test set score: 0.8472744360902256\n"],"name":"stdout"}]},{"metadata":{"id":"fKcEnL3eRRI5","colab_type":"text"},"cell_type":"markdown","source":["Holy overfitting, Batman! Overfitting is a known problem when using bag of words, since it basically involves throwing a massive number of features at a model – some of those features (in this case, word frequencies) will capture noise in the training set. Since overfitting is also a known problem with Random Forests, the divergence between training score and test score is expected.\n","\n","\n","## BoW with Logistic Regression\n","\n","Let's try a technique with some protection against overfitting due to extraneous features – logistic regression with ridge regularization (from ridge regression, also called L2 regularization)."]},{"metadata":{"id":"97JAOiGXRL8J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"fb9f23c4-3b67-4272-93d6-1c34c11e5c1e","executionInfo":{"status":"ok","timestamp":1551125287642,"user_tz":300,"elapsed":478,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","\n","lr = LogisticRegression()\n","train = lr.fit(X_train, y_train)\n","print(X_train.shape, y_train.shape)\n","print('Training set score:', lr.score(X_train, y_train))\n","print('\\nTest set score:', lr.score(X_test, y_test))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["(3190, 3062) (3190,)\n","Training set score: 0.9545454545454546\n","\n","Test set score: 0.8843984962406015\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"}]},{"metadata":{"id":"IENXbKvYRdAW","colab_type":"text"},"cell_type":"markdown","source":["Logistic regression performs a bit better than the random forest.  \n","\n","# BoW with Gradient Boosting\n","\n","And finally, let's see what gradient boosting can do:"]},{"metadata":{"id":"c0JUzG3IRTtB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"ef9201d6-39d3-4e55-ec6d-347fe0af22ad","executionInfo":{"status":"ok","timestamp":1551125378805,"user_tz":300,"elapsed":59808,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["clf = ensemble.GradientBoostingClassifier()\n","train = clf.fit(X_train, y_train)\n","\n","print('Training set score:', clf.score(X_train, y_train))\n","print('\\nTest set score:', clf.score(X_test, y_test))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Training set score: 0.8642633228840125\n","\n","Test set score: 0.8529135338345865\n"],"name":"stdout"}]},{"metadata":{"id":"NlWI2obQRkYp","colab_type":"text"},"cell_type":"markdown","source":["Looks like logistic regression is the winner, but there's room for improvement.\n","\n","# Same model, new inputs\n","\n","What if we feed the model a different novel by Jane Austen, like _Emma_?  Will it be able to distinguish Austen from Carroll with the same level of accuracy if we insert a different sample of Austen's writing?\n","\n","First, we need to process _Emma_ the same way we processed the other data, and combine it with the Alice data:"]},{"metadata":{"id":"YEbM81nvRgyp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2ebb35d3-2ec4-4c29-d20e-15e19eb75404","executionInfo":{"status":"ok","timestamp":1551125383148,"user_tz":300,"elapsed":269,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["# Clean the Emma data.\n","emma = gutenberg.raw('austen-emma.txt')\n","emma = re.sub(r'VOLUME \\w+', '', emma)\n","emma = re.sub(r'CHAPTER \\w+', '', emma)\n","emma = text_cleaner(emma)\n","print(emma[:100])"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to\n"],"name":"stdout"}]},{"metadata":{"id":"Hq6SqbYCRwZK","colab_type":"code","colab":{}},"cell_type":"code","source":["# Parse our cleaned data.\n","emma_doc = nlp(emma)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rVcQSzMGR1Ss","colab_type":"code","colab":{}},"cell_type":"code","source":["# Group into sentences.\n","persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n","emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]\n","\n","# Emma is quite long, let's cut it down to the same length as Alice.\n","emma_sents = emma_sents[0:len(alice_sents)]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YGGK13YrSDFq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"303dbd2d-e2b0-4ed2-b5da-0fa637941170","executionInfo":{"status":"ok","timestamp":1551125568458,"user_tz":300,"elapsed":406,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["emma_sentences = pd.DataFrame(emma_sents)\n","emma_sentences.head()"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>(Emma, Woodhouse, ,, handsome, ,, clever, ,, a...</td>\n","      <td>Austen</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>(She, was, the, youngest, of, the, two, daught...</td>\n","      <td>Austen</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>(Her, mother, had, died, too, long, ago, for, ...</td>\n","      <td>Austen</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>(Sixteen, years, had, Miss, Taylor, been, in, ...</td>\n","      <td>Austen</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>(Between, _, them)</td>\n","      <td>Austen</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                   0       1\n","0  (Emma, Woodhouse, ,, handsome, ,, clever, ,, a...  Austen\n","1  (She, was, the, youngest, of, the, two, daught...  Austen\n","2  (Her, mother, had, died, too, long, ago, for, ...  Austen\n","3  (Sixteen, years, had, Miss, Taylor, been, in, ...  Austen\n","4                                 (Between, _, them)  Austen"]},"metadata":{"tags":[]},"execution_count":17}]},{"metadata":{"id":"5mAooODsSHEb","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","emma = pd.DataFrame()\n","emma['text_sentence'] = emma_sentences[0]\n","emma['text_source'] = emma_sentences[1]\n","#String version of the spacy objects\n","emma['str_sentence'] = [_.text for _ in emma.text_sentence]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XaOPPNOkSiGL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":250},"outputId":"00ff89db-6fbe-49c1-b3b6-7efd32851a12","executionInfo":{"status":"ok","timestamp":1551125623672,"user_tz":300,"elapsed":283,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["X = vectorizer.fit_transform(emma['str_sentence'])\n","df2 =  pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n","\n","emma_bow = pd.concat([emma,df2], axis=1)\n","print(emma_bow.shape)\n","emma_bow.head()\n"],"execution_count":19,"outputs":[{"output_type":"stream","text":["(1669, 3065)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text_sentence</th>\n","      <th>text_source</th>\n","      <th>str_sentence</th>\n","      <th>'s</th>\n","      <th>-PRON-</th>\n","      <th>1</th>\n","      <th>`</th>\n","      <th>a</th>\n","      <th>abide</th>\n","      <th>ability</th>\n","      <th>...</th>\n","      <th>you</th>\n","      <th>you'd</th>\n","      <th>young</th>\n","      <th>your</th>\n","      <th>yours</th>\n","      <th>youth</th>\n","      <th>zeal</th>\n","      <th>zealand</th>\n","      <th>zealous</th>\n","      <th>zigzag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>(Emma, Woodhouse, ,, handsome, ,, clever, ,, a...</td>\n","      <td>Austen</td>\n","      <td>Emma Woodhouse, handsome, clever, and rich, wi...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>(She, was, the, youngest, of, the, two, daught...</td>\n","      <td>Austen</td>\n","      <td>She was the youngest of the two daughters of a...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>(Her, mother, had, died, too, long, ago, for, ...</td>\n","      <td>Austen</td>\n","      <td>Her mother had died too long ago for her to ha...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>(Sixteen, years, had, Miss, Taylor, been, in, ...</td>\n","      <td>Austen</td>\n","      <td>Sixteen years had Miss Taylor been in Mr. Wood...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>(Between, _, them)</td>\n","      <td>Austen</td>\n","      <td>Between _them</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 3065 columns</p>\n","</div>"],"text/plain":["                                       text_sentence text_source  \\\n","0  (Emma, Woodhouse, ,, handsome, ,, clever, ,, a...      Austen   \n","1  (She, was, the, youngest, of, the, two, daught...      Austen   \n","2  (Her, mother, had, died, too, long, ago, for, ...      Austen   \n","3  (Sixteen, years, had, Miss, Taylor, been, in, ...      Austen   \n","4                                 (Between, _, them)      Austen   \n","\n","                                        str_sentence  's  -PRON-  1  `  a  \\\n","0  Emma Woodhouse, handsome, clever, and rich, wi...   0       0  0  0  0   \n","1  She was the youngest of the two daughters of a...   0       0  0  0  0   \n","2  Her mother had died too long ago for her to ha...   0       0  0  0  0   \n","3  Sixteen years had Miss Taylor been in Mr. Wood...   0       0  0  0  0   \n","4                                      Between _them   0       0  0  0  0   \n","\n","   abide  ability   ...    you  you'd  young  your  yours  youth  zeal  \\\n","0      0        0   ...      0      0      0     0      0      0     0   \n","1      0        0   ...      0      0      0     0      0      0     0   \n","2      0        0   ...      0      0      0     0      0      0     0   \n","3      0        0   ...      0      0      0     0      0      0     0   \n","4      0        0   ...      0      0      0     0      0      0     0   \n","\n","   zealand  zealous  zigzag  \n","0        0        0       0  \n","1        0        0       0  \n","2        0        0       0  \n","3        0        0       0  \n","4        0        0       0  \n","\n","[5 rows x 3065 columns]"]},"metadata":{"tags":[]},"execution_count":19}]},{"metadata":{"id":"kLPsMjeCSrG0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":176},"outputId":"92700d9b-c078-4927-9e2f-68be1ca20640","executionInfo":{"status":"ok","timestamp":1551125655808,"user_tz":300,"elapsed":416,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["# Combine the Emma sentence data with the Alice data from the test set.\n","X_Emma_test = np.concatenate((\n","    X_train[y_train[y_train=='Carroll'].index],\n","    emma_bow.drop(['text_sentence','text_source', 'str_sentence'], 1)\n","), axis=0)\n","y_Emma_test = pd.concat([y_train[y_train=='Carroll'],\n","                         pd.Series(['Austen'] * emma_bow.shape[0])])\n","\n","# Model.\n","print('\\nTest set score:', lr.score(X_Emma_test, y_Emma_test))\n","lr_Emma_predicted = lr.predict(X_Emma_test)\n","pd.crosstab(y_Emma_test, lr_Emma_predicted)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["\n","Test set score: 0.6923937360178971\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>col_0</th>\n","      <th>Austen</th>\n","      <th>Carroll</th>\n","    </tr>\n","    <tr>\n","      <th>row_0</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Austen</th>\n","      <td>1536</td>\n","      <td>133</td>\n","    </tr>\n","    <tr>\n","      <th>Carroll</th>\n","      <td>692</td>\n","      <td>321</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["col_0    Austen  Carroll\n","row_0                   \n","Austen     1536      133\n","Carroll     692      321"]},"metadata":{"tags":[]},"execution_count":20}]},{"metadata":{"id":"S0r2AzcbS4Cd","colab_type":"text"},"cell_type":"markdown","source":["Well look at that!  NLP approaches are generally effective on the same type of material as they were trained on. It looks like this model is actually able to differentiate multiple works by Austen from Alice in Wonderland.  Now the question is whether the model is very good at identifying Austen, or very good at identifying Alice in Wonderland, or both...\n","\n","# Challenge 0:\n","\n","Recall that the logistic regression model's best performance on the test set was 93%.  See what you can do to improve performance.  Suggested avenues of investigation include: Other modeling techniques (SVM?), making more features that take advantage of the spaCy information (include grammar, phrases, POS, etc), making sentence-level features (number of words, amount of punctuation), or including contextual information (length of previous and next sentences, words repeated from one sentence to the next, etc), and anything else your heart desires.  Make sure to design your models on the test set, or use cross_validation with multiple folds, and see if you can get accuracy above 90%.  "]},{"metadata":{"id":"CT_gO2peSy7D","colab_type":"code","colab":{}},"cell_type":"code","source":["# Adding in column for the number of words in each sentence\n","word_counts['sent_length'] = word_counts.text_sentence.map(lambda x: len(x))\n","\n","sentences = word_counts.text_sentence"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4OeLFml1T0OE","colab_type":"code","colab":{}},"cell_type":"code","source":["# Creating list of number of adverbs in each sentence\n","sentences = word_counts.text_sentence\n","adv_count = []\n","for sent in sentences:\n","    advs = 0\n","    for token in sent:\n","        if token.pos_ == 'ADV':\n","            advs +=1\n","    adv_count.append(advs)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Dj-usOQ7T8cE","colab_type":"code","colab":{}},"cell_type":"code","source":["# Creating column for number of adverbs per sentence\n","word_counts['adv_count'] = adv_count\n","\n","# Creating list of number of verbs in each sentence\n","verb_count = []\n","for sent in sentences:\n","    verb = 0\n","    for token in sent:\n","        if token.pos_ == 'VERB':\n","            verb +=1\n","    verb_count.append(verb)\n","\n","word_counts['verb_count'] = verb_count\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0vhq77tbUH4D","colab_type":"code","colab":{}},"cell_type":"code","source":["# Creating list of number of nouns in each sentence\n","noun_count = []\n","for sent in sentences:\n","    noun = 0\n","    for token in sent:\n","        if token.pos_ == 'NOUN':\n","            noun +=1\n","    noun_count.append(noun)\n","\n","word_counts['noun_count'] = noun_count"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wvRkj4OEUMVj","colab_type":"code","colab":{}},"cell_type":"code","source":["# Creating list of number of punctuation marks in each sentence\n","punc_count = []\n","for sent in sentences:\n","    punc = 0\n","    for token in sent:\n","        if token.pos_ == 'PUNCT':\n","            punc +=1\n","    punc_count.append(punc)\n","\n","word_counts['punc_count'] = punc_count"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3MNhYuTKUSLW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":284},"outputId":"004ab48d-0cab-4355-984b-f6498d1434f5","executionInfo":{"status":"ok","timestamp":1551126060536,"user_tz":300,"elapsed":309,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["word_counts.head()"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text_sentence</th>\n","      <th>text_source</th>\n","      <th>str_sentence</th>\n","      <th>'s</th>\n","      <th>-PRON-</th>\n","      <th>1</th>\n","      <th>`</th>\n","      <th>a</th>\n","      <th>abide</th>\n","      <th>ability</th>\n","      <th>...</th>\n","      <th>youth</th>\n","      <th>zeal</th>\n","      <th>zealand</th>\n","      <th>zealous</th>\n","      <th>zigzag</th>\n","      <th>sent_length</th>\n","      <th>adv_count</th>\n","      <th>verb_count</th>\n","      <th>noun_count</th>\n","      <th>punc_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n","      <td>Carroll</td>\n","      <td>Alice was beginning to get very tired of sitti...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>67</td>\n","      <td>3</td>\n","      <td>13</td>\n","      <td>12</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>(So, she, was, considering, in, her, own, mind...</td>\n","      <td>Carroll</td>\n","      <td>So she was considering in her own mind (as wel...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>63</td>\n","      <td>7</td>\n","      <td>11</td>\n","      <td>8</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n","      <td>Carroll</td>\n","      <td>There was nothing so VERY remarkable in that; ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>33</td>\n","      <td>6</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>(Oh, dear, !)</td>\n","      <td>Carroll</td>\n","      <td>Oh dear!</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>(I, shall, be, late, !, ')</td>\n","      <td>Carroll</td>\n","      <td>I shall be late!'</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 3070 columns</p>\n","</div>"],"text/plain":["                                       text_sentence text_source  \\\n","0  (Alice, was, beginning, to, get, very, tired, ...     Carroll   \n","1  (So, she, was, considering, in, her, own, mind...     Carroll   \n","2  (There, was, nothing, so, VERY, remarkable, in...     Carroll   \n","3                                      (Oh, dear, !)     Carroll   \n","4                         (I, shall, be, late, !, ')     Carroll   \n","\n","                                        str_sentence  's  -PRON-  1  `  a  \\\n","0  Alice was beginning to get very tired of sitti...   0       0  0  0  0   \n","1  So she was considering in her own mind (as wel...   0       0  0  0  0   \n","2  There was nothing so VERY remarkable in that; ...   0       0  0  0  0   \n","3                                           Oh dear!   0       0  0  0  0   \n","4                                  I shall be late!'   0       0  0  0  0   \n","\n","   abide  ability     ...      youth  zeal  zealand  zealous  zigzag  \\\n","0      0        0     ...          0     0        0        0       0   \n","1      0        0     ...          0     0        0        0       0   \n","2      0        0     ...          0     0        0        0       0   \n","3      0        0     ...          0     0        0        0       0   \n","4      0        0     ...          0     0        0        0       0   \n","\n","   sent_length  adv_count  verb_count  noun_count  punc_count  \n","0           67          3          13          12          10  \n","1           63          7          11           8           7  \n","2           33          6           5           2           4  \n","3            3          0           0           0           1  \n","4            6          0           2           0           2  \n","\n","[5 rows x 3070 columns]"]},"metadata":{"tags":[]},"execution_count":26}]},{"metadata":{"id":"ho-dKiycUVwi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"c254fe06-080c-4a76-ae08-453c26f8b9cc","executionInfo":{"status":"ok","timestamp":1551126108948,"user_tz":300,"elapsed":1641,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["Y = word_counts['text_source']\n","X = np.array(word_counts.drop(['text_sentence','text_source','str_sentence'], 1))\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, \n","                                                    Y,\n","                                                    test_size=0.4,\n","                                                    random_state=0)\n","lr = LogisticRegression()\n","train = lr.fit(X_train, y_train)\n","print(X_train.shape, y_train.shape)\n","print('Training set score:', lr.score(X_train, y_train))\n","print('\\nTest set score:', lr.score(X_test, y_test))"],"execution_count":28,"outputs":[{"output_type":"stream","text":["(3190, 3067) (3190,)\n","Training set score: 0.9557993730407524\n","\n","Test set score: 0.8900375939849624\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"}]},{"metadata":{"id":"ZSCeUSl3UpBj","colab_type":"text"},"cell_type":"markdown","source":["We can see that the accuracy increased slightly using the number of words, verbs, adverbs, nouns and punctuations. "]},{"metadata":{"id":"MTuwyFTYVBr-","colab_type":"text"},"cell_type":"markdown","source":["\n","# Challenge 1:\n","Find out whether your new model is good at identifying Alice in Wonderland vs any other work, Persuasion vs any other work, or Austen vs any other work.  This will involve pulling a new book from the Project Gutenberg corpus (print(gutenberg.fileids()) for a list) and processing it.\n","\n","Record your work for each challenge in a notebook and submit it below."]},{"metadata":{"id":"GhkAkrQLUfub","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"a74f9fcd-c2a1-4b0e-bad8-b479b10e57d7","executionInfo":{"status":"ok","timestamp":1551126325783,"user_tz":300,"elapsed":355,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["print(gutenberg.fileids())"],"execution_count":29,"outputs":[{"output_type":"stream","text":["['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"],"name":"stdout"}]},{"metadata":{"id":"Bh-ax_sCVcoF","colab_type":"text"},"cell_type":"markdown","source":["### Using Parents by Edgeworth."]},{"metadata":{"id":"g2dfNg8kVWgM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"86ae225c-69c4-468d-d6f1-68c73fd58515","executionInfo":{"status":"ok","timestamp":1551126466459,"user_tz":300,"elapsed":275,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["#Can this differentiate Paradise from Alice in Wonderland?\n","\n","parents = gutenberg.raw('edgeworth-parents.txt')\n","parents = re.sub(r'VOLUME \\w+', '', parents)\n","parents = re.sub(r'CHAPTER \\w+', '', parents)\n","parents = text_cleaner(parents)\n","print(type(parents))\n","print(parents[:100])"],"execution_count":31,"outputs":[{"output_type":"stream","text":["<class 'str'>\n","THE ORPHANS. Near the ruins of the castle of Rossmore, in Ireland, is a small cabin, in which there \n"],"name":"stdout"}]},{"metadata":{"id":"RwzbUZBmVpvM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8e4b917b-b543-448e-e58e-6f9e982b98d1","executionInfo":{"status":"ok","timestamp":1551126471382,"user_tz":300,"elapsed":337,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["len(parents)"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["903807"]},"metadata":{"tags":[]},"execution_count":32}]},{"metadata":{"id":"bqG1tiNnV6DT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"35556304-9e97-4947-994d-08ed27f559a0","executionInfo":{"status":"ok","timestamp":1551126481296,"user_tz":300,"elapsed":292,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["len(alice)"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["141708"]},"metadata":{"tags":[]},"execution_count":33}]},{"metadata":{"id":"Wjoo_mVOV8fD","colab_type":"code","colab":{}},"cell_type":"code","source":["parentslen = md[0:len(alice)]\n","parents_doc = nlp(parentslen)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9KVx3AyCWHdF","colab_type":"code","colab":{}},"cell_type":"code","source":["parents_sents = [[sent, \"Edgeworth\"] for sent in parents_doc.sents]\n","alice_sents = alice_sents[0:len(parents_sents)]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kVOQHSeibc1-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"00a735e7-0663-4266-886d-845e0c40ebda","executionInfo":{"status":"ok","timestamp":1551127990692,"user_tz":300,"elapsed":296,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["len(alice_sents)"],"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1196"]},"metadata":{"tags":[]},"execution_count":64}]},{"metadata":{"id":"xlT1b5j1bgQB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"1f0f35e7-119d-427b-ed6d-aa9ff6980561","executionInfo":{"status":"ok","timestamp":1551127991889,"user_tz":300,"elapsed":318,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["len(parents_sents)"],"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1196"]},"metadata":{"tags":[]},"execution_count":65}]},{"metadata":{"id":"O3Ekp_g7WtjF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"5eb04be3-21e0-4c23-d082-c9cd6149c3db","executionInfo":{"status":"ok","timestamp":1551127993854,"user_tz":300,"elapsed":307,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["parents_sentences = pd.DataFrame(parents_sents)\n","parents_sentences.head()\n"],"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>(THE, ORPHANS, .)</td>\n","      <td>Edgeworth</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>(Near, the, ruins, of, the, castle, of, Rossmo...</td>\n","      <td>Edgeworth</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>(As, long, as, she, was, able, to, work, ,, sh...</td>\n","      <td>Edgeworth</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>(Mary, was, at, this, time, about, twelve, yea...</td>\n","      <td>Edgeworth</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>(One, evening, she, was, sitting, at, the, foo...</td>\n","      <td>Edgeworth</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                   0          1\n","0                                  (THE, ORPHANS, .)  Edgeworth\n","1  (Near, the, ruins, of, the, castle, of, Rossmo...  Edgeworth\n","2  (As, long, as, she, was, able, to, work, ,, sh...  Edgeworth\n","3  (Mary, was, at, this, time, about, twelve, yea...  Edgeworth\n","4  (One, evening, she, was, sitting, at, the, foo...  Edgeworth"]},"metadata":{"tags":[]},"execution_count":66}]},{"metadata":{"id":"y81j0DTiXO8N","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","parent = pd.DataFrame()\n","parent['text_sentence'] = parents_sentences[0]\n","parent['text_source'] = parents_sentences[1]\n","#String version of the spacy objects\n","parent['str_sentence'] = [_.text for _ in parent.text_sentence]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xF-FN9TyXSz-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":250},"outputId":"9ce4f8a8-2888-4509-f13b-992fff6786f1","executionInfo":{"status":"ok","timestamp":1551128000000,"user_tz":300,"elapsed":265,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["X = vectorizer.fit_transform(parent['str_sentence'])\n","df3 =  pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n","\n","parent_bow = pd.concat([parent,df3], axis=1)\n","print(parent_bow.shape)\n","parent_bow.head()\n"],"execution_count":68,"outputs":[{"output_type":"stream","text":["(1196, 3065)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text_sentence</th>\n","      <th>text_source</th>\n","      <th>str_sentence</th>\n","      <th>'s</th>\n","      <th>-PRON-</th>\n","      <th>1</th>\n","      <th>`</th>\n","      <th>a</th>\n","      <th>abide</th>\n","      <th>ability</th>\n","      <th>...</th>\n","      <th>you</th>\n","      <th>you'd</th>\n","      <th>young</th>\n","      <th>your</th>\n","      <th>yours</th>\n","      <th>youth</th>\n","      <th>zeal</th>\n","      <th>zealand</th>\n","      <th>zealous</th>\n","      <th>zigzag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>(THE, ORPHANS, .)</td>\n","      <td>Edgeworth</td>\n","      <td>THE ORPHANS.</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>(Near, the, ruins, of, the, castle, of, Rossmo...</td>\n","      <td>Edgeworth</td>\n","      <td>Near the ruins of the castle of Rossmore, in I...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>(As, long, as, she, was, able, to, work, ,, sh...</td>\n","      <td>Edgeworth</td>\n","      <td>As long as she was able to work, she was very ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>(Mary, was, at, this, time, about, twelve, yea...</td>\n","      <td>Edgeworth</td>\n","      <td>Mary was at this time about twelve years old.</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>(One, evening, she, was, sitting, at, the, foo...</td>\n","      <td>Edgeworth</td>\n","      <td>One evening she was sitting at the foot of her...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 3065 columns</p>\n","</div>"],"text/plain":["                                       text_sentence text_source  \\\n","0                                  (THE, ORPHANS, .)   Edgeworth   \n","1  (Near, the, ruins, of, the, castle, of, Rossmo...   Edgeworth   \n","2  (As, long, as, she, was, able, to, work, ,, sh...   Edgeworth   \n","3  (Mary, was, at, this, time, about, twelve, yea...   Edgeworth   \n","4  (One, evening, she, was, sitting, at, the, foo...   Edgeworth   \n","\n","                                        str_sentence  's  -PRON-  1  `  a  \\\n","0                                       THE ORPHANS.   0       0  0  0  0   \n","1  Near the ruins of the castle of Rossmore, in I...   0       0  0  0  0   \n","2  As long as she was able to work, she was very ...   0       0  0  0  0   \n","3      Mary was at this time about twelve years old.   0       0  0  0  0   \n","4  One evening she was sitting at the foot of her...   0       0  0  0  0   \n","\n","   abide  ability   ...    you  you'd  young  your  yours  youth  zeal  \\\n","0      0        0   ...      0      0      0     0      0      0     0   \n","1      0        0   ...      0      0      0     0      0      0     0   \n","2      0        0   ...      0      0      0     0      0      0     0   \n","3      0        0   ...      0      0      0     0      0      0     0   \n","4      0        0   ...      0      0      0     0      0      0     0   \n","\n","   zealand  zealous  zigzag  \n","0        0        0       0  \n","1        0        0       0  \n","2        0        0       0  \n","3        0        0       0  \n","4        0        0       0  \n","\n","[5 rows x 3065 columns]"]},"metadata":{"tags":[]},"execution_count":68}]},{"metadata":{"id":"cU2bnKDyXjb9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":227},"outputId":"1a0a265f-5198-4084-f77c-5d66230ae472","executionInfo":{"status":"ok","timestamp":1551128506607,"user_tz":300,"elapsed":958,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["# Identifying variables\n","X_parent = parent_bow.drop(['text_sentence','text_source','str_sentence'], 1)\n","y_parent = parent_bow.text_source\n","\n","alice_wc = word_counts[word_counts.text_source == 'Carroll']\n","X_alice = alice_wc.drop(['text_sentence','text_source','str_sentence'], 1)\n","y_alice = alice_wc.text_source[1196:]\n","\n","# Combine the Paradise sentence data with the Alice data from the test set.\n","X_pa = pd.concat([X_parent, X_alice], 0).dropna()\n","y_pa = pd.concat([y_parent, y_alice], 0).dropna()\n","\n","# Split into train and test sets\n","X_train_pa, X_test_pa, y_train_pa, y_test_pa = train_test_split(X_pa, y_pa, test_size=0.4, random_state=0)\n","\n","# Model\n","lr2 = LogisticRegression()\n","train = lr2.fit(X_train_pa, y_train_pa)\n","print('Train set score:', lr2.score(X_train_pa, y_train_pa))\n","print('\\nTest set score:', lr2.score(X_test_pa, y_test_pa))\n","lr_pa_predicted = lr2.predict(X_test_pa)\n","pd.crosstab(y_test_pa, lr_pa_predicted)"],"execution_count":79,"outputs":[{"output_type":"stream","text":["Train set score: 0.9100899100899101\n","\n","Test set score: 0.812874251497006\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>col_0</th>\n","      <th>Carroll</th>\n","      <th>Edgeworth</th>\n","    </tr>\n","    <tr>\n","      <th>text_source</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Carroll</th>\n","      <td>89</td>\n","      <td>104</td>\n","    </tr>\n","    <tr>\n","      <th>Edgeworth</th>\n","      <td>21</td>\n","      <td>454</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["col_0        Carroll  Edgeworth\n","text_source                    \n","Carroll           89        104\n","Edgeworth         21        454"]},"metadata":{"tags":[]},"execution_count":79}]},{"metadata":{"id":"62JEHJSJYDzs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":227},"outputId":"956ad3c3-6cef-4598-c17a-d0c80d7caf8f","executionInfo":{"status":"ok","timestamp":1551128820202,"user_tz":300,"elapsed":1411,"user":{"displayName":"Bhavya Bharath","photoUrl":"https://lh6.googleusercontent.com/-uNO2Lz8t174/AAAAAAAAAAI/AAAAAAAAGaQ/14ktZd7VfUY/s64/photo.jpg","userId":"00841795119622634199"}}},"cell_type":"code","source":["# Identifying variables\n","X_parent = parent_bow.drop(['text_sentence','text_source','str_sentence'], 1)\n","y_parent = parent_bow.text_source\n","\n","persuasion_wc = word_counts[word_counts.text_source == 'Austen']\n","X_persuasion = persuasion_wc.drop(['text_sentence','text_source','str_sentence'], 1)\n","y_persuasion = persuasion_wc.text_source[1196:]\n","\n","# Combine the Paradise sentence data with the Alice data from the test set.\n","X_pp = pd.concat([X_parent, X_persuasion], 0).dropna()\n","y_pp = pd.concat([y_parent, y_persuasion], 0).dropna()\n","\n","# Split into train and test sets\n","X_train_pp, X_test_pp, y_train_pp, y_test_pp = train_test_split(X_pp, y_pp, test_size=0.4, random_state=0)\n","\n","# Model\n","lr2 = LogisticRegression()\n","train = lr2.fit(X_train_pp, y_train_pp)\n","print('Train set score:', lr2.score(X_train_pp, y_train_pp))\n","print('\\nTest set score:', lr2.score(X_test_pp, y_test_pp))\n","lr_pp_predicted = lr2.predict(X_test_pp)\n","pd.crosstab(y_test_pp, lr_pp_predicted)"],"execution_count":80,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Train set score: 0.8876199177706715\n","\n","Test set score: 0.6821917808219178\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>col_0</th>\n","      <th>Austen</th>\n","      <th>Edgeworth</th>\n","    </tr>\n","    <tr>\n","      <th>text_source</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Austen</th>\n","      <td>819</td>\n","      <td>138</td>\n","    </tr>\n","    <tr>\n","      <th>Edgeworth</th>\n","      <td>326</td>\n","      <td>177</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["col_0        Austen  Edgeworth\n","text_source                   \n","Austen          819        138\n","Edgeworth       326        177"]},"metadata":{"tags":[]},"execution_count":80}]},{"metadata":{"id":"n-ure8qXfmR8","colab_type":"text"},"cell_type":"markdown","source":["Looks like the model did an ok job distinguishing the different models"]},{"metadata":{"id":"4pbomuHWe3OP","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}